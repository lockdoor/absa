# ADR-006: Auto-Labeling Service Design

**Status:** Proposed

**Date:** 2025-12-23

**Deciders:** ML Engineering Team, Backend Team

**Related:** ADR-001, ADR-003

---

## Context and Problem Statement

à¸£à¸°à¸šà¸šà¸•à¹‰à¸­à¸‡à¸à¸²à¸£ labels à¸ªà¸³à¸«à¸£à¸±à¸š reviews à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸Šà¹‰à¹ƒà¸™ training à¹à¸¥à¸° validation à¹à¸•à¹ˆà¸à¸²à¸£ label à¸”à¹‰à¸§à¸¢à¸¡à¸™à¸¸à¸©à¸¢à¹Œà¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸™à¸²à¸™à¹à¸¥à¸°à¸¡à¸µà¸„à¹ˆà¸²à¹ƒà¸Šà¹‰à¸ˆà¹ˆà¸²à¸¢à¸ªà¸¹à¸‡ à¹€à¸£à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸£à¸°à¸šà¸š auto-labeling à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰ LLM (Gemini Flash) à¸Šà¹ˆà¸§à¸¢à¹ƒà¸™à¸à¸²à¸£ label reviews à¹‚à¸”à¸¢à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´

**Requirements:**
- Fetch reviews à¸—à¸µà¹ˆà¸¢à¸±à¸‡à¹„à¸¡à¹ˆà¸¡à¸µ labels
- à¸ªà¹ˆà¸‡ review à¹„à¸› LLM API (Gemini Flash) à¹€à¸žà¸·à¹ˆà¸­ label aspects à¹à¸¥à¸° sentiments
- Update labels à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆ database
- à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¸ªà¸¥à¸±à¸š LLM providers (Gemini, GPT, Claude)
- à¸ˆà¸±à¸”à¸à¸²à¸£ errors à¹à¸¥à¸° retries
- Track labeling quality à¹à¸¥à¸° cost

---

## Decision Drivers

* **Automation:** à¸¥à¸” manual labeling effort
* **Cost Efficiency:** à¹ƒà¸Šà¹‰ cheaper LLM (Flash models)
* **Flexibility:** à¸ªà¸¥à¸±à¸š LLM providers à¹„à¸”à¹‰
* **Quality Control:** Validate labels à¸à¹ˆà¸­à¸™ save
* **Observability:** Track metrics à¹à¸¥à¸° costs
* **Scalability:** Process multiple reviews in batch

---

## Design: Service-First Approach

### Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LabelingService (Orchestrator)   â”‚ â† High-level workflow
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - fetch_and_label_batch()           â”‚
â”‚  - validate_and_save()               â”‚
â”‚  - monitor_quality()                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Review   â”‚    â”‚ Labeling       â”‚
    â”‚Repositoryâ”‚    â”‚ Provider       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (Strategy)     â”‚
           â†“        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â†“
    â”‚ Dataset  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ (Factory)â”‚    â”‚ Gemini API     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ GPT API        â”‚
           â†“        â”‚ Claude API     â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ Database â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Service Layer Design

### 1. LabelingService (Main Orchestrator)

**Responsibility:** Orchestrate the complete auto-labeling workflow

**Methods:**
```python
class LabelingService:
    def label_batch(batch_id: int, limit: int) -> LabelingResult
    def label_unlabeled_reviews(batch_size: int, max_batches: int)
    def validate_labels(labels: Dict) -> ValidationResult
    def save_labels(review_id: int, labels: Dict, metadata: Dict)
    def get_labeling_stats(batch_id: int) -> Stats
```

**Workflow:**
1. Fetch unlabeled reviews from repository
2. Send to LLM provider for labeling
3. Validate labels
4. Save to database
5. Track metrics and costs

---

### 2. LabelingProvider (Strategy Pattern)

**Responsibility:** Abstract LLM API calls

**Interface:**
```python
class BaseLabelingProvider(ABC):
    @abstractmethod
    def label_review(text: str, aspects: List[str]) -> LabelResult
    
    @abstractmethod
    def label_batch(texts: List[str], aspects: List[str]) -> List[LabelResult]
    
    def get_cost() -> float
    def get_model_info() -> ModelInfo
```

**Implementations:**
- GeminiLabelingProvider
- GPTLabelingProvider  
- ClaudeLabelingProvider
- HumanLabelingProvider (fallback)

---

### 3. ReviewRepository

**Responsibility:** Data access for reviews

**Methods:**
```python
class ReviewRepository:
    def get_unlabeled_reviews(batch_id: int, limit: int) -> DataFrame
    def get_review_by_id(review_id: int) -> Review
    def update_labels(review_id: int, labels: Dict, metadata: Dict)
    def get_labeling_progress(batch_id: int) -> Progress
    def get_reviews_for_validation(sample_size: int) -> DataFrame
```

---

## Data Flow

```
1. Service Request
   â†“
2. LabelingService.label_batch(batch_id=123, limit=100)
   â†“
3. ReviewRepository.get_unlabeled_reviews(batch_id, limit)
   â†“ Returns DataFrame
4. For each review:
   LabelingProvider.label_review(text, aspects)
   â†“ Returns LabelResult
5. ValidationService.validate(labels)
   â†“ Returns ValidationResult
6. If valid:
   ReviewRepository.update_labels(review_id, labels, metadata)
   â†“
7. Return LabelingResult (success, failed, costs)
```

---

## Label Schema

### Input to LLM
```json
{
  "text": "à¸­à¸²à¸«à¸²à¸£à¸­à¸£à¹ˆà¸­à¸¢à¸¡à¸²à¸ à¹à¸•à¹ˆà¸šà¸£à¸´à¸à¸²à¸£à¸Šà¹‰à¸²à¹„à¸›à¸«à¸™à¹ˆà¸­à¸¢",
  "aspects": ["food", "service", "price", "ambiance"],
  "instructions": "Extract mentioned aspects and their sentiments"
}
```

### Output from LLM (labels field in DB)
```json
{
  "aspects": {
    "food": {
      "mentioned": true,
      "sentiment": "positive",
      "confidence": 0.95,
      "snippet": "à¸­à¸²à¸«à¸²à¸£à¸­à¸£à¹ˆà¸­à¸¢à¸¡à¸²à¸"
    },
    "service": {
      "mentioned": true,
      "sentiment": "negative",
      "confidence": 0.88,
      "snippet": "à¸šà¸£à¸´à¸à¸²à¸£à¸Šà¹‰à¸²à¹„à¸›à¸«à¸™à¹ˆà¸­à¸¢"
    },
    "price": {
      "mentioned": false,
      "sentiment": null,
      "confidence": null,
      "snippet": null
    }
  },
  "overall_sentiment": "neutral",
  "labeling_metadata": {
    "provider": "gemini-flash-2.0",
    "timestamp": "2025-12-23T10:00:00Z",
    "cost": 0.0001,
    "processing_time_ms": 150,
    "validation_passed": true
  }
}
```

---

## Implementation Priority

### Phase 1: Core Service (Week 1)
```python
services/
â””â”€â”€ labeling/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ labeling_service.py      # Main orchestrator
    â”œâ”€â”€ validation_service.py     # Label validation
    â””â”€â”€ providers/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ base_provider.py      # Abstract base
        â””â”€â”€ gemini_provider.py    # Gemini implementation
```

### Phase 2: Repository Layer (Week 1)
```python
repositories/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_repository.py
â””â”€â”€ review_repository.py         # CRUD for reviews
```

### Phase 3: Additional Providers (Week 2)
```python
services/labeling/providers/
â”œâ”€â”€ gpt_provider.py
â”œâ”€â”€ claude_provider.py
â””â”€â”€ human_provider.py            # Manual labeling interface
```

### Phase 4: Monitoring & Analytics (Week 2)
```python
services/labeling/
â”œâ”€â”€ monitoring_service.py        # Track metrics
â””â”€â”€ cost_tracking_service.py     # Cost analysis
```

---

## Example Usage

### Simple Labeling
```python
# Setup
gemini_provider = GeminiLabelingProvider(api_key="...")
review_repo = ReviewRepository(dataset=create_dataset(client))
labeling_service = LabelingService(
    repository=review_repo,
    provider=gemini_provider
)

# Label a batch
result = labeling_service.label_batch(
    batch_id=123,
    limit=100
)

print(f"Labeled: {result.success_count}")
print(f"Failed: {result.failed_count}")
print(f"Total cost: ${result.total_cost}")
```

### Batch Processing
```python
# Label all unlabeled reviews
stats = labeling_service.label_unlabeled_reviews(
    batch_size=50,
    max_batches=20
)
```

### Switch Provider
```python
# Switch to GPT for comparison
gpt_provider = GPTLabelingProvider(api_key="...")
labeling_service.set_provider(gpt_provider)
```

---

## Error Handling Strategy

### Retry Logic
```python
@retry(max_attempts=3, backoff=exponential)
def label_review(text: str):
    # API call
    pass
```

### Fallback Strategy
1. Primary: Gemini Flash (cheap, fast)
2. Fallback 1: GPT-4o-mini
3. Fallback 2: Flag for human labeling

### Error Types
- **API Error:** Retry with backoff
- **Rate Limit:** Queue and process later
- **Invalid Response:** Log and flag for review
- **Validation Failed:** Save with warning flag

---

## Cost Management

### Cost Tracking
```python
class CostTracker:
    def track_call(provider: str, tokens: int, cost: float)
    def get_daily_cost() -> float
    def get_cost_per_review() -> float
    def alert_if_budget_exceeded(threshold: float)
```

### Budget Limits
```python
labeling_service.set_daily_budget(max_usd=10.0)
labeling_service.set_cost_alert(threshold=0.80)  # 80% of budget
```

---

## Quality Control

### Validation Rules
```python
class LabelValidator:
    def validate_label(label: Dict) -> ValidationResult:
        # 1. Check required fields
        # 2. Validate sentiment values
        # 3. Check confidence thresholds
        # 4. Verify aspect mentions
        # 5. Detect inconsistencies
```

### Human Review Triggers
- Confidence < 0.70
- Contradictory sentiments
- Unusual aspect combinations
- First N samples of each batch (for quality check)

---

## Monitoring Metrics

### Key Metrics
- **Throughput:** Reviews labeled per hour
- **Cost:** USD per review
- **Quality:** Validation pass rate
- **Latency:** API response time
- **Error Rate:** Failed labeling attempts

### Dashboards
```python
monitoring_service.get_metrics(batch_id) â†’ {
    "total_reviews": 1000,
    "labeled": 950,
    "failed": 50,
    "avg_confidence": 0.87,
    "total_cost": 2.50,
    "avg_latency_ms": 180
}
```

---

## Security & Privacy

### API Key Management
```python
# Use environment variables
GEMINI_API_KEY=os.getenv("GEMINI_API_KEY")

# Or secret manager
from cloud_secrets import get_secret
api_key = get_secret("gemini-api-key")
```

### Data Privacy
- âœ… Reviews sent to LLM are public data only
- âœ… No PII in prompts
- âœ… Comply with LLM provider ToS
- âœ… Optional: Self-hosted LLM for sensitive data

---

## Future Enhancements

1. **Active Learning:** Prioritize uncertain samples for human review
2. **Multi-Model Ensemble:** Combine predictions from multiple LLMs
3. **Fine-tuned Models:** Train custom models on labeled data
4. **Real-time Labeling:** Stream processing for live data
5. **Feedback Loop:** Learn from human corrections

---

## Decision Outcome

**Chosen Approach:** Service-First Design with Strategy Pattern

**Justification:**
- âœ… Clean separation: Service orchestrates, Repository accesses data
- âœ… Flexible: Easy to switch LLM providers
- âœ… Testable: Mock providers and repositories
- âœ… Scalable: Process in batches
- âœ… Observable: Built-in monitoring and cost tracking

### Next Steps
1. âœ… Design service interfaces (this document)
2. ðŸ”„ Implement LabelingService skeleton
3. ðŸ”„ Implement GeminiLabelingProvider
4. ðŸ”„ Implement ReviewRepository
5. â³ Add validation and error handling
6. â³ Add monitoring and cost tracking
7. â³ Integration testing
8. â³ Production deployment

---

## Links

* [ADR-001: Overall Architecture](001-overall-architecture.md)
* [ADR-003: Service Layer Design](003-service-layer-design.md)
* [ERD Documentation](../ERD.md)

---

## Notes

- Start with Gemini Flash (cheapest for testing)
- Monitor costs closely in initial phase
- Human validation for first 100 labels to establish baseline
- Consider rate limits and quotas
- Keep fallback to manual labeling always available
